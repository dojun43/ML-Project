{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Library"
      ],
      "metadata": {
        "id": "iLU3o4VAYhKP"
      },
      "id": "iLU3o4VAYhKP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cd1f4ff",
      "metadata": {
        "id": "9cd1f4ff"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os \n",
        "from datetime import datetime\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from itertools import product\n",
        "import pickle\n",
        "\n",
        "from numba import cuda\n",
        "import gc\n",
        "\n",
        "os.chdir(r'C:\\Users\\Nyoths\\Desktop\\한강')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU 설정"
      ],
      "metadata": {
        "id": "EgxirogEYkmn"
      },
      "id": "EgxirogEYkmn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42f30a88-90f6-4263-af40-dcc9ad1e8f55",
      "metadata": {
        "id": "42f30a88-90f6-4263-af40-dcc9ad1e8f55"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "  except RuntimeError as e:\n",
        "    # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "yIJ2RhKZYr8z"
      },
      "id": "yIJ2RhKZYr8z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fdc297b",
      "metadata": {
        "id": "8fdc297b"
      },
      "outputs": [],
      "source": [
        "def read_path_csv(path):\n",
        "    file_list = os.listdir(path)\n",
        "    file_list_py = [file for file in file_list if file.endswith('.csv')]\n",
        "\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "    for file in file_list_py:\n",
        "        data = pd.read_csv(path + file)\n",
        "        df = pd.concat([df, data], ignore_index=True)\n",
        "\n",
        "        print('read', file)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df6deabc",
      "metadata": {
        "id": "df6deabc",
        "outputId": "fd89cbd5-a216-4c0a-cc9d-26ad01134c20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "read rf_2012.csv\n",
            "read rf_2013.csv\n",
            "read rf_2014.csv\n",
            "read rf_2015.csv\n",
            "read rf_2016.csv\n",
            "read rf_2017.csv\n",
            "read rf_2018.csv\n",
            "read rf_2019.csv\n",
            "read rf_2020.csv\n",
            "read rf_2021.csv\n",
            "read rf_2022.csv\n"
          ]
        }
      ],
      "source": [
        "rf_data = read_path_csv('rf_data/')\n",
        "rf_data = rf_data.sort_values('ymdhm', ascending = True).reset_index(drop=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c693c4d9",
      "metadata": {
        "id": "c693c4d9",
        "outputId": "de32438c-9457-4fa8-b6e4-c85a5946d6f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "read data_2012.csv\n",
            "read data_2013.csv\n",
            "read data_2014.csv\n",
            "read data_2015.csv\n",
            "read data_2016.csv\n",
            "read data_2017.csv\n",
            "read data_2018.csv\n",
            "read data_2019.csv\n",
            "read data_2020.csv\n",
            "read data_2021.csv\n",
            "read data_2022.csv\n"
          ]
        }
      ],
      "source": [
        "water_data = read_path_csv('water_data/')\n",
        "water_data = water_data.sort_values('ymdhm', ascending = True).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전처리"
      ],
      "metadata": {
        "id": "BuWn_S1KYxYp"
      },
      "id": "BuWn_S1KYxYp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 특정일 결측치 제거 함수\n",
        "def drop_date_range(df, start, end):\n",
        "    start = datetime.strptime(start, \"%Y-%m-%d\")\n",
        "    end = datetime.strptime(end, \"%Y-%m-%d\")\n",
        "    date_list = [date.strftime(\"%Y-%m-%d\") for date in pd.date_range(start, periods=(end - start).days + 1)]\n",
        "\n",
        "    drop_idx = df['ymd'].isin(date_list)\n",
        "    df = df[~drop_idx]\n",
        "\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "drjD6y-ZYHXM"
      },
      "id": "drjD6y-ZYHXM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46ed04f8",
      "metadata": {
        "id": "46ed04f8"
      },
      "outputs": [],
      "source": [
        "df = pd.merge(rf_data, water_data, on='ymdhm') #데이터 프레임 결합\n",
        "df['ymd'] = df['ymdhm'].str[0:10] # 날짜 컬럼 추가\n",
        "df['year'] = df['ymdhm'].str[0:4] # 연도 컬럼 추가\n",
        "df['month'] = df['ymdhm'].str[5:7] # 월 컬럼 추가\n",
        "df['day'] = df['ymdhm'].str[8:9] # 일 컬럼 추가\n",
        "df = df.drop(['fw_1018680'],axis=1) # fw_1018680 컬럼 제거\n",
        "df = drop_date_range(df, '2021-10-14', '2021-10-31') # tide_level 결측치 부분 제거\n",
        "df = df.interpolate(method=\"linear\") # 선형보간\n",
        "df = df[:-6912] # test data 제거  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68bccfc5",
      "metadata": {
        "id": "68bccfc5"
      },
      "outputs": [],
      "source": [
        "# MinMaxScaler 생성\n",
        "\n",
        "X_col_list = ['tototf', 'tide_level',\n",
        "              'wl_1018662', 'fw_1018662', 'wl_1018680', 'wl_1018683',\n",
        "              'fw_1018683', 'wl_1019630', 'fw_1019630']\n",
        "\n",
        "y_col_list = ['wl_1018662', 'wl_1018680', 'wl_1018683', 'wl_1019630']\n",
        "\n",
        "data = df.loc[:,X_col_list]\n",
        "target = df.loc[:,y_col_list]\n",
        "\n",
        "data_scaler = MinMaxScaler().fit(data)\n",
        "target_scaler = MinMaxScaler().fit(target)\n",
        "\n",
        "data = []\n",
        "target = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def create_data_set(df, window_size, X_col_list, y_col_list, data_scaler, target_scaler):\n",
        "    data_list = []\n",
        "    target_list = []\n",
        "\n",
        "    year_list = ['2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019','2020', '2021', '2022']\n",
        "\n",
        "    for year in year_list:\n",
        "        df_year = df[df.year == year].reset_index(drop=True)\n",
        "\n",
        "        data = df_year.loc[:,X_col_list]\n",
        "        target = df_year.loc[:,y_col_list]\n",
        "\n",
        "        data = data_scaler.transform(data)\n",
        "        target = target_scaler.transform(target)\n",
        "\n",
        "        for i in range(window_size, len(data)):\n",
        "            data_list.append(np.array(data[i-window_size:i]))\n",
        "            target_list.append(np.array(target[i]))\n",
        "\n",
        "    data_list = np.array(data_list)\n",
        "    target_list = np.array(target_list)\n",
        "\n",
        "    return data_list, target_list"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "H7Uj0JGBYHXO"
      },
      "id": "H7Uj0JGBYHXO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92749f42",
      "metadata": {
        "id": "92749f42",
        "outputId": "a5473c32-1b23-4fd2-e8be-d9954263a221"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape: (265248, 144, 9) Y shape: (265248, 4)\n"
          ]
        }
      ],
      "source": [
        "# data 생성\n",
        "window_size = 144 # 1일 데이터 input\n",
        "X_col_list = ['tototf', 'tide_level',\n",
        "              'wl_1018662', 'fw_1018662', 'wl_1018680', 'wl_1018683',\n",
        "              'fw_1018683', 'wl_1019630', 'fw_1019630']\n",
        "\n",
        "y_col_list = ['wl_1018662', 'wl_1018680', 'wl_1018683', 'wl_1019630']\n",
        "\n",
        "data_scaled, target_scaled = create_data_set(df, window_size, X_col_list, y_col_list, data_scaler, target_scaler)\n",
        "\n",
        "print('X shape:', data_scaled.shape, 'Y shape:', target_scaled.shape)\n",
        "\n",
        "# 스케일러는 따로 저장. -> 모델 예측시 써야하니까.\n",
        "with open('data_scaler_samemodel_opt.pickle', 'wb') as f:\n",
        "    pickle.dump(data_scaler, f, pickle.HIGHEST_PROTOCOL)\n",
        "with open('target_scaler_samemodel_opt.pickle', 'wb') as f:\n",
        "    pickle.dump(target_scaler, f, pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a28e503f",
      "metadata": {
        "id": "a28e503f"
      },
      "outputs": [],
      "source": [
        "# train set, test set 분할 \n",
        "x_train, x_test, y_train, y_test = train_test_split(data_scaled, target_scaled, test_size=0.3, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95790d46",
      "metadata": {
        "id": "95790d46",
        "outputId": "00c142e6-1982-499a-f418-38759a22d4c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train: (185673, 144, 9) y_train: (185673, 4)\n",
            "x_test: (79575, 144, 9) y_test: (79575, 4)\n"
          ]
        }
      ],
      "source": [
        "# train set, test set 분할 후 shape \n",
        "x_train_shape = np.shape(x_train)\n",
        "y_train_shape = np.shape(y_train)\n",
        "x_test_shape = np.shape(x_test)\n",
        "y_test_shape = np.shape(y_test)\n",
        "\n",
        "print('x_train:',x_train_shape, 'y_train:',y_train_shape)\n",
        "print('x_test:',x_test_shape, 'y_test:',y_test_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e3e6d67",
      "metadata": {
        "id": "5e3e6d67"
      },
      "source": [
        "# LSTM "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c586c0a",
      "metadata": {
        "id": "8c586c0a"
      },
      "outputs": [],
      "source": [
        "def make_lstm(lstm_layers, dense_layers, dropout, input_shape = (144,9), output_shape = 4):\n",
        "    model = Sequential()\n",
        "    \n",
        "    for i, layer_size in enumerate(lstm_layers,1):\n",
        "        if i ==1 :\n",
        "            model.add(LSTM(layer_size, input_shape=input_shape, return_sequences = True))\n",
        "        else:\n",
        "            model.add(LSTM(layer_size, return_sequences = True))\n",
        "            model.add(LSTM(layer_size))\n",
        "    \n",
        "    for layer_size in dense_layers:\n",
        "        model.add(Dense(layer_size, activation='swish'))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Dense(dense_layers[-1], activation='swish'))\n",
        "    model.add(Dense(4))\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1da7c24b",
      "metadata": {
        "id": "1da7c24b",
        "outputId": "291a6812-feec-4c45-9cb3-18f1d0f93bcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "param gird length: 36\n"
          ]
        }
      ],
      "source": [
        "#grid params\n",
        "lstm_layer_opts = [(256,256),(256,256),(256,256)]\n",
        "dense_layer_opts = [(256,256),(256,256)]\n",
        "dropout_opts = [0.15,0.15]\n",
        "batch_size_opts = [64,64,64]\n",
        "\n",
        "param_gird = list(product(lstm_layer_opts, dense_layer_opts, dropout_opts, batch_size_opts))\n",
        "print('param gird length:', len(param_gird))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0f0f1f9",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "b0f0f1f9",
        "outputId": "be8b7c04-207c-497e-eaf6-a61d32d460b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_15 (LSTM)              (None, 144, 256)          272384    \n",
            "                                                                 \n",
            " lstm_16 (LSTM)              (None, 144, 256)          525312    \n",
            "                                                                 \n",
            " lstm_17 (LSTM)              (None, 256)               525312    \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 4)                 1028      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,521,412\n",
            "Trainable params: 1,521,412\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/500\n",
            "2902/2902 [==============================] - 133s 45ms/step - loss: 1.6663e-04 - val_loss: 4.2951e-05 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "2902/2902 [==============================] - 130s 45ms/step - loss: 5.1763e-05 - val_loss: 3.0947e-05 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 3.8933e-05 - val_loss: 1.4419e-05 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "2902/2902 [==============================] - 125s 43ms/step - loss: 3.9627e-05 - val_loss: 1.4895e-04 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 3.2411e-05 - val_loss: 3.9916e-05 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "2902/2902 [==============================] - 129s 45ms/step - loss: 2.9675e-05 - val_loss: 2.7175e-05 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.5041e-05 - val_loss: 4.3723e-06 - lr: 2.0000e-04\n",
            "Epoch 8/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.4986e-05 - val_loss: 5.2639e-06 - lr: 2.0000e-04\n",
            "Epoch 9/500\n",
            "2902/2902 [==============================] - 122s 42ms/step - loss: 1.4284e-05 - val_loss: 6.3540e-06 - lr: 2.0000e-04\n",
            "Epoch 10/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.4085e-05 - val_loss: 8.7881e-06 - lr: 2.0000e-04\n",
            "Epoch 11/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.4031e-05 - val_loss: 1.3006e-05 - lr: 2.0000e-04\n",
            "Epoch 12/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.1824e-05 - val_loss: 3.6119e-06 - lr: 4.0000e-05\n",
            "Epoch 13/500\n",
            "2902/2902 [==============================] - 120s 42ms/step - loss: 1.1544e-05 - val_loss: 3.7603e-06 - lr: 4.0000e-05\n",
            "Epoch 14/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.1404e-05 - val_loss: 4.0894e-06 - lr: 4.0000e-05\n",
            "Epoch 15/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.1395e-05 - val_loss: 3.6518e-06 - lr: 4.0000e-05\n",
            "Epoch 16/500\n",
            "2902/2902 [==============================] - 122s 42ms/step - loss: 1.1294e-05 - val_loss: 3.6113e-06 - lr: 4.0000e-05\n",
            "Epoch 17/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.0994e-05 - val_loss: 3.4727e-06 - lr: 8.0000e-06\n",
            "Epoch 18/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.0859e-05 - val_loss: 3.4321e-06 - lr: 8.0000e-06\n",
            "Epoch 19/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.0871e-05 - val_loss: 3.4068e-06 - lr: 8.0000e-06\n",
            "Epoch 20/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.0942e-05 - val_loss: 3.4629e-06 - lr: 8.0000e-06\n",
            "Epoch 21/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.0824e-05 - val_loss: 3.3799e-06 - lr: 8.0000e-06\n",
            "Epoch 22/500\n",
            "2902/2902 [==============================] - 119s 41ms/step - loss: 1.0658e-05 - val_loss: 3.3288e-06 - lr: 1.6000e-06\n",
            "Epoch 23/500\n",
            "2902/2902 [==============================] - 120s 41ms/step - loss: 1.0722e-05 - val_loss: 3.3728e-06 - lr: 1.6000e-06\n",
            "Epoch 24/500\n",
            "2902/2902 [==============================] - 120s 41ms/step - loss: 1.0751e-05 - val_loss: 3.3324e-06 - lr: 1.6000e-06\n",
            "Epoch 25/500\n",
            "2902/2902 [==============================] - 120s 41ms/step - loss: 1.0680e-05 - val_loss: 3.3252e-06 - lr: 1.6000e-06\n",
            "Epoch 26/500\n",
            "2902/2902 [==============================] - 120s 41ms/step - loss: 1.0656e-05 - val_loss: 3.3295e-06 - lr: 1.6000e-06\n",
            "Epoch 27/500\n",
            "2902/2902 [==============================] - 120s 41ms/step - loss: 1.0664e-05 - val_loss: 3.3262e-06 - lr: 3.2000e-07\n",
            "Epoch 28/500\n",
            "2902/2902 [==============================] - 120s 41ms/step - loss: 1.0565e-05 - val_loss: 3.3391e-06 - lr: 3.2000e-07\n",
            "Epoch 29/500\n",
            "2902/2902 [==============================] - 120s 41ms/step - loss: 1.0649e-05 - val_loss: 3.3333e-06 - lr: 3.2000e-07\n",
            "Epoch 30/500\n",
            "2902/2902 [==============================] - 120s 41ms/step - loss: 1.0680e-05 - val_loss: 3.3335e-06 - lr: 3.2000e-07\n",
            "Epoch 31/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.0618e-05 - val_loss: 3.3392e-06 - lr: 3.2000e-07\n",
            "Epoch 32/500\n",
            "2902/2902 [==============================] - 120s 41ms/step - loss: 1.0690e-05 - val_loss: 3.3311e-06 - lr: 6.4000e-08\n",
            "Epoch 33/500\n",
            "2902/2902 [==============================] - 120s 41ms/step - loss: 1.0715e-05 - val_loss: 3.3355e-06 - lr: 6.4000e-08\n",
            "Epoch 34/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.0615e-05 - val_loss: 3.3282e-06 - lr: 6.4000e-08\n",
            "Epoch 35/500\n",
            "2902/2902 [==============================] - 120s 41ms/step - loss: 1.0689e-05 - val_loss: 3.3284e-06 - lr: 6.4000e-08\n",
            "Epoch 36/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.0697e-05 - val_loss: 3.3271e-06 - lr: 6.4000e-08\n",
            "Epoch 37/500\n",
            "2902/2902 [==============================] - 120s 41ms/step - loss: 1.0655e-05 - val_loss: 3.3263e-06 - lr: 1.2800e-08\n",
            "Epoch 38/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.0736e-05 - val_loss: 3.3266e-06 - lr: 1.2800e-08\n",
            "Epoch 39/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.0647e-05 - val_loss: 3.3264e-06 - lr: 1.2800e-08\n",
            "Epoch 40/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.0645e-05 - val_loss: 3.3263e-06 - lr: 1.2800e-08\n",
            "5803/5803 [==============================] - 83s 14ms/step\n",
            "2487/2487 [==============================] - 35s 14ms/step\n",
            "\n",
            "                     model  lstm_layer dense_layer  dropout  batch_size  \\\n",
            "0  256same_model_opt_lstm1  (256, 256)  (256, 256)     0.15          64   \n",
            "\n",
            "   train_RMSE  test_RMSE  \n",
            "0    1.365419   1.407495  \n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_18 (LSTM)              (None, 144, 256)          272384    \n",
            "                                                                 \n",
            " lstm_19 (LSTM)              (None, 144, 256)          525312    \n",
            "                                                                 \n",
            " lstm_20 (LSTM)              (None, 256)               525312    \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 4)                 1028      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,521,412\n",
            "Trainable params: 1,521,412\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/500\n",
            "2902/2902 [==============================] - 124s 42ms/step - loss: 1.8400e-04 - val_loss: 4.8633e-05 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "2902/2902 [==============================] - 120s 41ms/step - loss: 5.7007e-05 - val_loss: 2.8049e-04 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "2902/2902 [==============================] - 120s 41ms/step - loss: 4.3688e-05 - val_loss: 2.0119e-05 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 3.9026e-05 - val_loss: 9.7873e-06 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "2902/2902 [==============================] - 120s 41ms/step - loss: 3.8674e-05 - val_loss: 1.3135e-05 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "2902/2902 [==============================] - 120s 42ms/step - loss: 3.0161e-05 - val_loss: 2.2956e-05 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "2902/2902 [==============================] - 120s 41ms/step - loss: 1.6113e-05 - val_loss: 1.1968e-05 - lr: 2.0000e-04\n",
            "Epoch 8/500\n",
            "2902/2902 [==============================] - 120s 41ms/step - loss: 1.5403e-05 - val_loss: 6.9708e-06 - lr: 2.0000e-04\n",
            "Epoch 9/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.5080e-05 - val_loss: 5.3724e-06 - lr: 2.0000e-04\n",
            "Epoch 10/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.4682e-05 - val_loss: 8.3823e-06 - lr: 2.0000e-04\n",
            "Epoch 11/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.4613e-05 - val_loss: 6.4639e-06 - lr: 2.0000e-04\n",
            "Epoch 12/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.2353e-05 - val_loss: 3.6907e-06 - lr: 4.0000e-05\n",
            "Epoch 13/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.2146e-05 - val_loss: 4.4953e-06 - lr: 4.0000e-05\n",
            "Epoch 14/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.2099e-05 - val_loss: 3.3034e-06 - lr: 4.0000e-05\n",
            "Epoch 15/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.2001e-05 - val_loss: 4.4854e-06 - lr: 4.0000e-05\n",
            "Epoch 16/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.1976e-05 - val_loss: 3.5731e-06 - lr: 4.0000e-05\n",
            "Epoch 17/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.1321e-05 - val_loss: 3.2211e-06 - lr: 8.0000e-06\n",
            "Epoch 18/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.1430e-05 - val_loss: 3.2210e-06 - lr: 8.0000e-06\n",
            "Epoch 19/500\n",
            "2902/2902 [==============================] - 122s 42ms/step - loss: 1.1373e-05 - val_loss: 3.2836e-06 - lr: 8.0000e-06\n",
            "Epoch 20/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.1382e-05 - val_loss: 3.3289e-06 - lr: 8.0000e-06\n",
            "Epoch 21/500\n",
            "2902/2902 [==============================] - 122s 42ms/step - loss: 1.1304e-05 - val_loss: 3.2354e-06 - lr: 8.0000e-06\n",
            "Epoch 22/500\n",
            "2902/2902 [==============================] - 122s 42ms/step - loss: 1.1218e-05 - val_loss: 3.1850e-06 - lr: 1.6000e-06\n",
            "Epoch 23/500\n",
            "2902/2902 [==============================] - 122s 42ms/step - loss: 1.1224e-05 - val_loss: 3.1797e-06 - lr: 1.6000e-06\n",
            "Epoch 24/500\n",
            "2902/2902 [==============================] - 122s 42ms/step - loss: 1.1185e-05 - val_loss: 3.1810e-06 - lr: 1.6000e-06\n",
            "Epoch 25/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.1104e-05 - val_loss: 3.1816e-06 - lr: 1.6000e-06\n",
            "Epoch 26/500\n",
            "2902/2902 [==============================] - 122s 42ms/step - loss: 1.1188e-05 - val_loss: 3.2125e-06 - lr: 1.6000e-06\n",
            "Epoch 27/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.1202e-05 - val_loss: 3.1854e-06 - lr: 3.2000e-07\n",
            "Epoch 28/500\n",
            "2902/2902 [==============================] - 122s 42ms/step - loss: 1.1153e-05 - val_loss: 3.1825e-06 - lr: 3.2000e-07\n",
            "Epoch 29/500\n",
            "2902/2902 [==============================] - 122s 42ms/step - loss: 1.1095e-05 - val_loss: 3.1768e-06 - lr: 3.2000e-07\n",
            "Epoch 30/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.1251e-05 - val_loss: 3.1754e-06 - lr: 3.2000e-07\n",
            "Epoch 31/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.1098e-05 - val_loss: 3.1751e-06 - lr: 3.2000e-07\n",
            "Epoch 32/500\n",
            "2902/2902 [==============================] - 121s 42ms/step - loss: 1.1121e-05 - val_loss: 3.1739e-06 - lr: 6.4000e-08\n",
            "Epoch 33/500\n",
            "2902/2902 [==============================] - 122s 42ms/step - loss: 1.1113e-05 - val_loss: 3.1727e-06 - lr: 6.4000e-08\n",
            "Epoch 34/500\n",
            "2902/2902 [==============================] - 129s 44ms/step - loss: 1.1190e-05 - val_loss: 3.1713e-06 - lr: 6.4000e-08\n",
            "Epoch 35/500\n",
            "2902/2902 [==============================] - 130s 45ms/step - loss: 1.1181e-05 - val_loss: 3.1725e-06 - lr: 6.4000e-08\n",
            "Epoch 36/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.1143e-05 - val_loss: 3.1726e-06 - lr: 6.4000e-08\n",
            "Epoch 37/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 1.1259e-05 - val_loss: 3.1706e-06 - lr: 1.2800e-08\n",
            "Epoch 38/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 1.1169e-05 - val_loss: 3.1709e-06 - lr: 1.2800e-08\n",
            "Epoch 39/500\n",
            "2902/2902 [==============================] - 125s 43ms/step - loss: 1.1163e-05 - val_loss: 3.1714e-06 - lr: 1.2800e-08\n",
            "Epoch 40/500\n",
            "2902/2902 [==============================] - 126s 43ms/step - loss: 1.1169e-05 - val_loss: 3.1709e-06 - lr: 1.2800e-08\n",
            "Epoch 41/500\n",
            "2902/2902 [==============================] - 126s 43ms/step - loss: 1.1156e-05 - val_loss: 3.1710e-06 - lr: 1.2800e-08\n",
            "Epoch 42/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 1.1002e-05 - val_loss: 3.1710e-06 - lr: 1.0000e-08\n",
            "Epoch 43/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 1.1084e-05 - val_loss: 3.1708e-06 - lr: 1.0000e-08\n",
            "Epoch 44/500\n",
            "2902/2902 [==============================] - 125s 43ms/step - loss: 1.1073e-05 - val_loss: 3.1705e-06 - lr: 1.0000e-08\n",
            "Epoch 45/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 1.1248e-05 - val_loss: 3.1702e-06 - lr: 1.0000e-08\n",
            "Epoch 46/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 1.1228e-05 - val_loss: 3.1708e-06 - lr: 1.0000e-08\n",
            "Epoch 47/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 1.1138e-05 - val_loss: 3.1709e-06 - lr: 1.0000e-08\n",
            "Epoch 48/500\n",
            "2902/2902 [==============================] - 125s 43ms/step - loss: 1.1056e-05 - val_loss: 3.1706e-06 - lr: 1.0000e-08\n",
            "Epoch 49/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 1.1204e-05 - val_loss: 3.1703e-06 - lr: 1.0000e-08\n",
            "Epoch 50/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 1.1116e-05 - val_loss: 3.1707e-06 - lr: 1.0000e-08\n",
            "Epoch 51/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 1.1186e-05 - val_loss: 3.1705e-06 - lr: 1.0000e-08\n",
            "Epoch 52/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 1.1153e-05 - val_loss: 3.1707e-06 - lr: 1.0000e-08\n",
            "Epoch 53/500\n",
            "2902/2902 [==============================] - 127s 44ms/step - loss: 1.1073e-05 - val_loss: 3.1715e-06 - lr: 1.0000e-08\n",
            "Epoch 54/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.1190e-05 - val_loss: 3.1711e-06 - lr: 1.0000e-08\n",
            "Epoch 55/500\n",
            "2902/2902 [==============================] - 126s 44ms/step - loss: 1.1120e-05 - val_loss: 3.1707e-06 - lr: 1.0000e-08\n",
            "Epoch 56/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 1.1150e-05 - val_loss: 3.1705e-06 - lr: 1.0000e-08\n",
            "Epoch 57/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 1.1166e-05 - val_loss: 3.1705e-06 - lr: 1.0000e-08\n",
            "Epoch 58/500\n",
            "2902/2902 [==============================] - 125s 43ms/step - loss: 1.0998e-05 - val_loss: 3.1709e-06 - lr: 1.0000e-08\n",
            "Epoch 59/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 1.1160e-05 - val_loss: 3.1708e-06 - lr: 1.0000e-08\n",
            "Epoch 60/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 1.1155e-05 - val_loss: 3.1709e-06 - lr: 1.0000e-08\n",
            "5803/5803 [==============================] - 86s 15ms/step\n",
            "2487/2487 [==============================] - 36s 15ms/step\n",
            "\n",
            "                     model  lstm_layer dense_layer  dropout  batch_size  \\\n",
            "0  256same_model_opt_lstm2  (256, 256)  (256, 256)     0.15          64   \n",
            "\n",
            "   train_RMSE  test_RMSE  \n",
            "0    1.388371   1.383931  \n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_21 (LSTM)              (None, 144, 256)          272384    \n",
            "                                                                 \n",
            " lstm_22 (LSTM)              (None, 144, 256)          525312    \n",
            "                                                                 \n",
            " lstm_23 (LSTM)              (None, 256)               525312    \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 4)                 1028      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,521,412\n",
            "Trainable params: 1,521,412\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/500\n",
            "2902/2902 [==============================] - 128s 43ms/step - loss: 1.8313e-04 - val_loss: 5.0328e-05 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "2902/2902 [==============================] - 125s 43ms/step - loss: 5.0819e-05 - val_loss: 6.3686e-05 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 4.4224e-05 - val_loss: 1.9734e-05 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 3.6758e-05 - val_loss: 9.5282e-06 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 3.4749e-05 - val_loss: 3.3466e-05 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 3.2379e-05 - val_loss: 6.2355e-06 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "2902/2902 [==============================] - 125s 43ms/step - loss: 1.5650e-05 - val_loss: 4.8604e-06 - lr: 2.0000e-04\n",
            "Epoch 8/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 1.5375e-05 - val_loss: 4.7468e-06 - lr: 2.0000e-04\n",
            "Epoch 9/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 1.4837e-05 - val_loss: 7.7601e-06 - lr: 2.0000e-04\n",
            "Epoch 10/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 1.4596e-05 - val_loss: 4.4446e-06 - lr: 2.0000e-04\n",
            "Epoch 11/500\n",
            "2902/2902 [==============================] - 125s 43ms/step - loss: 1.4719e-05 - val_loss: 4.6939e-06 - lr: 2.0000e-04\n",
            "Epoch 12/500\n",
            "2902/2902 [==============================] - 125s 43ms/step - loss: 1.2270e-05 - val_loss: 3.3205e-06 - lr: 4.0000e-05\n",
            "Epoch 13/500\n",
            "2902/2902 [==============================] - 125s 43ms/step - loss: 1.2190e-05 - val_loss: 3.5167e-06 - lr: 4.0000e-05\n",
            "Epoch 14/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 1.2077e-05 - val_loss: 4.0834e-06 - lr: 4.0000e-05\n",
            "Epoch 15/500\n",
            "2902/2902 [==============================] - 125s 43ms/step - loss: 1.1810e-05 - val_loss: 3.4057e-06 - lr: 4.0000e-05\n",
            "Epoch 16/500\n",
            "2902/2902 [==============================] - 127s 44ms/step - loss: 1.1872e-05 - val_loss: 3.5281e-06 - lr: 4.0000e-05\n",
            "Epoch 17/500\n",
            "2902/2902 [==============================] - 124s 43ms/step - loss: 1.1340e-05 - val_loss: 3.2881e-06 - lr: 8.0000e-06\n",
            "Epoch 18/500\n",
            "2902/2902 [==============================] - 126s 43ms/step - loss: 1.1318e-05 - val_loss: 3.2335e-06 - lr: 8.0000e-06\n",
            "Epoch 19/500\n",
            "2902/2902 [==============================] - 125s 43ms/step - loss: 1.1304e-05 - val_loss: 3.2719e-06 - lr: 8.0000e-06\n",
            "Epoch 20/500\n",
            "2902/2902 [==============================] - 125s 43ms/step - loss: 1.1246e-05 - val_loss: 3.1779e-06 - lr: 8.0000e-06\n",
            "Epoch 21/500\n",
            "2902/2902 [==============================] - 128s 44ms/step - loss: 1.1293e-05 - val_loss: 3.2638e-06 - lr: 8.0000e-06\n",
            "Epoch 22/500\n",
            "2902/2902 [==============================] - 125s 43ms/step - loss: 1.1213e-05 - val_loss: 3.2043e-06 - lr: 1.6000e-06\n",
            "Epoch 23/500\n",
            "2902/2902 [==============================] - 127s 44ms/step - loss: 1.1214e-05 - val_loss: 3.1730e-06 - lr: 1.6000e-06\n",
            "Epoch 24/500\n",
            "2902/2902 [==============================] - 127s 44ms/step - loss: 1.1034e-05 - val_loss: 3.1774e-06 - lr: 1.6000e-06\n",
            "Epoch 25/500\n",
            "2902/2902 [==============================] - 129s 45ms/step - loss: 1.1151e-05 - val_loss: 3.1776e-06 - lr: 1.6000e-06\n",
            "Epoch 26/500\n",
            "2902/2902 [==============================] - 130s 45ms/step - loss: 1.1178e-05 - val_loss: 3.1711e-06 - lr: 1.6000e-06\n",
            "Epoch 27/500\n",
            "2902/2902 [==============================] - 128s 44ms/step - loss: 1.1124e-05 - val_loss: 3.1643e-06 - lr: 3.2000e-07\n",
            "Epoch 28/500\n",
            "2902/2902 [==============================] - 126s 43ms/step - loss: 1.1054e-05 - val_loss: 3.1612e-06 - lr: 3.2000e-07\n",
            "Epoch 29/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.1003e-05 - val_loss: 3.1620e-06 - lr: 3.2000e-07\n",
            "Epoch 30/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.1109e-05 - val_loss: 3.1740e-06 - lr: 3.2000e-07\n",
            "Epoch 31/500\n",
            "2902/2902 [==============================] - 130s 45ms/step - loss: 1.1142e-05 - val_loss: 3.1704e-06 - lr: 3.2000e-07\n",
            "Epoch 32/500\n",
            "2902/2902 [==============================] - 129s 44ms/step - loss: 1.1048e-05 - val_loss: 3.1640e-06 - lr: 6.4000e-08\n",
            "Epoch 33/500\n",
            "2902/2902 [==============================] - 132s 45ms/step - loss: 1.1061e-05 - val_loss: 3.1633e-06 - lr: 6.4000e-08\n",
            "Epoch 34/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.1040e-05 - val_loss: 3.1627e-06 - lr: 6.4000e-08\n",
            "Epoch 35/500\n",
            "2902/2902 [==============================] - 186s 64ms/step - loss: 1.1103e-05 - val_loss: 3.1643e-06 - lr: 6.4000e-08\n",
            "Epoch 36/500\n",
            "2902/2902 [==============================] - 166s 57ms/step - loss: 1.1132e-05 - val_loss: 3.1621e-06 - lr: 6.4000e-08\n",
            "Epoch 37/500\n",
            "2902/2902 [==============================] - 150s 52ms/step - loss: 1.1172e-05 - val_loss: 3.1609e-06 - lr: 1.2800e-08\n",
            "Epoch 38/500\n",
            "2902/2902 [==============================] - 147s 51ms/step - loss: 1.1125e-05 - val_loss: 3.1614e-06 - lr: 1.2800e-08\n",
            "Epoch 39/500\n",
            "2902/2902 [==============================] - 148s 51ms/step - loss: 1.0988e-05 - val_loss: 3.1617e-06 - lr: 1.2800e-08\n",
            "Epoch 40/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 1.1054e-05 - val_loss: 3.1615e-06 - lr: 1.2800e-08\n",
            "Epoch 41/500\n",
            "2902/2902 [==============================] - 136s 47ms/step - loss: 1.1002e-05 - val_loss: 3.1612e-06 - lr: 1.2800e-08\n",
            "Epoch 42/500\n",
            "2902/2902 [==============================] - 136s 47ms/step - loss: 1.1155e-05 - val_loss: 3.1616e-06 - lr: 1.0000e-08\n",
            "Epoch 43/500\n",
            "2902/2902 [==============================] - 138s 47ms/step - loss: 1.1236e-05 - val_loss: 3.1616e-06 - lr: 1.0000e-08\n",
            "Epoch 44/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 1.1038e-05 - val_loss: 3.1615e-06 - lr: 1.0000e-08\n",
            "Epoch 45/500\n",
            "2902/2902 [==============================] - 136s 47ms/step - loss: 1.1111e-05 - val_loss: 3.1618e-06 - lr: 1.0000e-08\n",
            "Epoch 46/500\n",
            "2902/2902 [==============================] - 132s 45ms/step - loss: 1.1047e-05 - val_loss: 3.1615e-06 - lr: 1.0000e-08\n",
            "Epoch 47/500\n",
            "2902/2902 [==============================] - 129s 44ms/step - loss: 1.1034e-05 - val_loss: 3.1616e-06 - lr: 1.0000e-08\n",
            "Epoch 48/500\n",
            "2902/2902 [==============================] - 127s 44ms/step - loss: 1.1057e-05 - val_loss: 3.1620e-06 - lr: 1.0000e-08\n",
            "Epoch 49/500\n",
            "2902/2902 [==============================] - 127s 44ms/step - loss: 1.1120e-05 - val_loss: 3.1619e-06 - lr: 1.0000e-08\n",
            "Epoch 50/500\n",
            "2902/2902 [==============================] - 127s 44ms/step - loss: 1.1065e-05 - val_loss: 3.1620e-06 - lr: 1.0000e-08\n",
            "Epoch 51/500\n",
            "2902/2902 [==============================] - 127s 44ms/step - loss: 1.1112e-05 - val_loss: 3.1619e-06 - lr: 1.0000e-08\n",
            "Epoch 52/500\n",
            "2902/2902 [==============================] - 128s 44ms/step - loss: 1.1126e-05 - val_loss: 3.1615e-06 - lr: 1.0000e-08\n",
            "5803/5803 [==============================] - 89s 15ms/step\n",
            "2487/2487 [==============================] - 37s 15ms/step\n",
            "\n",
            "                     model  lstm_layer dense_layer  dropout  batch_size  \\\n",
            "0  256same_model_opt_lstm3  (256, 256)  (256, 256)     0.15          64   \n",
            "\n",
            "   train_RMSE  test_RMSE  \n",
            "0    1.377241   1.382892  \n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_24 (LSTM)              (None, 144, 256)          272384    \n",
            "                                                                 \n",
            " lstm_25 (LSTM)              (None, 144, 256)          525312    \n",
            "                                                                 \n",
            " lstm_26 (LSTM)              (None, 256)               525312    \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 4)                 1028      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,521,412\n",
            "Trainable params: 1,521,412\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/500\n",
            "2902/2902 [==============================] - 132s 45ms/step - loss: 1.3757e-04 - val_loss: 1.8992e-05 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "2902/2902 [==============================] - 132s 46ms/step - loss: 5.4942e-05 - val_loss: 4.6671e-05 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "2902/2902 [==============================] - 136s 47ms/step - loss: 4.2964e-05 - val_loss: 2.4427e-05 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "2902/2902 [==============================] - 138s 47ms/step - loss: 3.8529e-05 - val_loss: 1.9399e-05 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 3.3325e-05 - val_loss: 4.1496e-05 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "2902/2902 [==============================] - 136s 47ms/step - loss: 2.9641e-05 - val_loss: 1.1764e-05 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "2902/2902 [==============================] - 142s 49ms/step - loss: 1.5185e-05 - val_loss: 8.0468e-06 - lr: 2.0000e-04\n",
            "Epoch 8/500\n",
            "2902/2902 [==============================] - 140s 48ms/step - loss: 1.5023e-05 - val_loss: 5.7734e-06 - lr: 2.0000e-04\n",
            "Epoch 9/500\n",
            "2902/2902 [==============================] - 140s 48ms/step - loss: 1.4532e-05 - val_loss: 4.0686e-06 - lr: 2.0000e-04\n",
            "Epoch 10/500\n",
            "2902/2902 [==============================] - 138s 48ms/step - loss: 1.4280e-05 - val_loss: 1.1940e-05 - lr: 2.0000e-04\n",
            "Epoch 11/500\n",
            "2902/2902 [==============================] - 139s 48ms/step - loss: 1.4144e-05 - val_loss: 3.9881e-06 - lr: 2.0000e-04\n",
            "Epoch 12/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 1.1906e-05 - val_loss: 3.8831e-06 - lr: 4.0000e-05\n",
            "Epoch 13/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.1660e-05 - val_loss: 3.6488e-06 - lr: 4.0000e-05\n",
            "Epoch 14/500\n",
            "2902/2902 [==============================] - 135s 46ms/step - loss: 1.1613e-05 - val_loss: 3.3966e-06 - lr: 4.0000e-05\n",
            "Epoch 15/500\n",
            "2902/2902 [==============================] - 139s 48ms/step - loss: 1.1631e-05 - val_loss: 3.7519e-06 - lr: 4.0000e-05\n",
            "Epoch 16/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.1489e-05 - val_loss: 3.2301e-06 - lr: 4.0000e-05\n",
            "Epoch 17/500\n",
            "2902/2902 [==============================] - 139s 48ms/step - loss: 1.1174e-05 - val_loss: 3.1723e-06 - lr: 8.0000e-06\n",
            "Epoch 18/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 1.0950e-05 - val_loss: 3.1704e-06 - lr: 8.0000e-06\n",
            "Epoch 19/500\n",
            "2902/2902 [==============================] - 132s 46ms/step - loss: 1.0935e-05 - val_loss: 3.1429e-06 - lr: 8.0000e-06\n",
            "Epoch 20/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.1056e-05 - val_loss: 3.2961e-06 - lr: 8.0000e-06\n",
            "Epoch 21/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.0951e-05 - val_loss: 3.1687e-06 - lr: 8.0000e-06\n",
            "Epoch 22/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.0810e-05 - val_loss: 3.1445e-06 - lr: 1.6000e-06\n",
            "Epoch 23/500\n",
            "2902/2902 [==============================] - 136s 47ms/step - loss: 1.0875e-05 - val_loss: 3.1203e-06 - lr: 1.6000e-06\n",
            "Epoch 24/500\n",
            "2902/2902 [==============================] - 130s 45ms/step - loss: 1.0821e-05 - val_loss: 3.1235e-06 - lr: 1.6000e-06\n",
            "Epoch 25/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.0822e-05 - val_loss: 3.1453e-06 - lr: 1.6000e-06\n",
            "Epoch 26/500\n",
            "2902/2902 [==============================] - 136s 47ms/step - loss: 1.0925e-05 - val_loss: 3.1293e-06 - lr: 1.6000e-06\n",
            "Epoch 27/500\n",
            "2902/2902 [==============================] - 132s 45ms/step - loss: 1.0768e-05 - val_loss: 3.1259e-06 - lr: 3.2000e-07\n",
            "Epoch 28/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 1.0787e-05 - val_loss: 3.1186e-06 - lr: 3.2000e-07\n",
            "Epoch 29/500\n",
            "2902/2902 [==============================] - 135s 47ms/step - loss: 1.0749e-05 - val_loss: 3.1235e-06 - lr: 3.2000e-07\n",
            "Epoch 30/500\n",
            "2902/2902 [==============================] - 139s 48ms/step - loss: 1.0712e-05 - val_loss: 3.1218e-06 - lr: 3.2000e-07\n",
            "Epoch 31/500\n",
            "2902/2902 [==============================] - 136s 47ms/step - loss: 1.0911e-05 - val_loss: 3.1179e-06 - lr: 3.2000e-07\n",
            "Epoch 32/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.0878e-05 - val_loss: 3.1184e-06 - lr: 6.4000e-08\n",
            "Epoch 33/500\n",
            "2902/2902 [==============================] - 135s 46ms/step - loss: 1.0784e-05 - val_loss: 3.1180e-06 - lr: 6.4000e-08\n",
            "Epoch 34/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.0684e-05 - val_loss: 3.1183e-06 - lr: 6.4000e-08\n",
            "Epoch 35/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.0801e-05 - val_loss: 3.1204e-06 - lr: 6.4000e-08\n",
            "Epoch 36/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.0714e-05 - val_loss: 3.1190e-06 - lr: 6.4000e-08\n",
            "Epoch 37/500\n",
            "2902/2902 [==============================] - 144s 50ms/step - loss: 1.0700e-05 - val_loss: 3.1197e-06 - lr: 1.2800e-08\n",
            "Epoch 38/500\n",
            "2902/2902 [==============================] - 143s 49ms/step - loss: 1.0729e-05 - val_loss: 3.1199e-06 - lr: 1.2800e-08\n",
            "Epoch 39/500\n",
            "2902/2902 [==============================] - 145s 50ms/step - loss: 1.0831e-05 - val_loss: 3.1190e-06 - lr: 1.2800e-08\n",
            "Epoch 40/500\n",
            "2902/2902 [==============================] - 138s 48ms/step - loss: 1.0878e-05 - val_loss: 3.1192e-06 - lr: 1.2800e-08\n",
            "Epoch 41/500\n",
            "2902/2902 [==============================] - 138s 48ms/step - loss: 1.0903e-05 - val_loss: 3.1190e-06 - lr: 1.2800e-08\n",
            "Epoch 42/500\n",
            "2902/2902 [==============================] - 140s 48ms/step - loss: 1.0779e-05 - val_loss: 3.1185e-06 - lr: 1.0000e-08\n",
            "Epoch 43/500\n",
            "2902/2902 [==============================] - 145s 50ms/step - loss: 1.0756e-05 - val_loss: 3.1188e-06 - lr: 1.0000e-08\n",
            "Epoch 44/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 1.0776e-05 - val_loss: 3.1191e-06 - lr: 1.0000e-08\n",
            "Epoch 45/500\n",
            "2902/2902 [==============================] - 135s 46ms/step - loss: 1.0760e-05 - val_loss: 3.1189e-06 - lr: 1.0000e-08\n",
            "Epoch 46/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.0922e-05 - val_loss: 3.1191e-06 - lr: 1.0000e-08\n",
            "5803/5803 [==============================] - 91s 16ms/step\n",
            "2487/2487 [==============================] - 41s 16ms/step\n",
            "\n",
            "                     model  lstm_layer dense_layer  dropout  batch_size  \\\n",
            "0  256same_model_opt_lstm4  (256, 256)  (256, 256)     0.15          64   \n",
            "\n",
            "   train_RMSE  test_RMSE  \n",
            "0    1.367921   1.373733  \n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_27 (LSTM)              (None, 144, 256)          272384    \n",
            "                                                                 \n",
            " lstm_28 (LSTM)              (None, 144, 256)          525312    \n",
            "                                                                 \n",
            " lstm_29 (LSTM)              (None, 256)               525312    \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 4)                 1028      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,521,412\n",
            "Trainable params: 1,521,412\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/500\n",
            "2902/2902 [==============================] - 135s 46ms/step - loss: 1.8108e-04 - val_loss: 3.3295e-05 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "2902/2902 [==============================] - 128s 44ms/step - loss: 5.6211e-05 - val_loss: 1.8302e-05 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "2902/2902 [==============================] - 130s 45ms/step - loss: 4.5431e-05 - val_loss: 7.0037e-05 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "2902/2902 [==============================] - 125s 43ms/step - loss: 4.1025e-05 - val_loss: 2.4789e-05 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "2902/2902 [==============================] - 129s 44ms/step - loss: 3.1909e-05 - val_loss: 1.1828e-05 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 3.3235e-05 - val_loss: 7.5767e-05 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "2902/2902 [==============================] - 129s 44ms/step - loss: 1.5681e-05 - val_loss: 6.8779e-06 - lr: 2.0000e-04\n",
            "Epoch 8/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.5198e-05 - val_loss: 9.2678e-06 - lr: 2.0000e-04\n",
            "Epoch 9/500\n",
            "2902/2902 [==============================] - 128s 44ms/step - loss: 1.4798e-05 - val_loss: 6.8552e-06 - lr: 2.0000e-04\n",
            "Epoch 10/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.4400e-05 - val_loss: 3.8706e-06 - lr: 2.0000e-04\n",
            "Epoch 11/500\n",
            "2902/2902 [==============================] - 129s 44ms/step - loss: 1.4119e-05 - val_loss: 4.3517e-06 - lr: 2.0000e-04\n",
            "Epoch 12/500\n",
            "2902/2902 [==============================] - 130s 45ms/step - loss: 1.2123e-05 - val_loss: 3.7648e-06 - lr: 4.0000e-05\n",
            "Epoch 13/500\n",
            "2902/2902 [==============================] - 128s 44ms/step - loss: 1.1932e-05 - val_loss: 4.1350e-06 - lr: 4.0000e-05\n",
            "Epoch 14/500\n",
            "2902/2902 [==============================] - 130s 45ms/step - loss: 1.1772e-05 - val_loss: 3.8590e-06 - lr: 4.0000e-05\n",
            "Epoch 15/500\n",
            "2902/2902 [==============================] - 127s 44ms/step - loss: 1.1615e-05 - val_loss: 3.5095e-06 - lr: 4.0000e-05\n",
            "Epoch 16/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.1607e-05 - val_loss: 3.5381e-06 - lr: 4.0000e-05\n",
            "Epoch 17/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.1257e-05 - val_loss: 3.2921e-06 - lr: 8.0000e-06\n",
            "Epoch 18/500\n",
            "2902/2902 [==============================] - 126s 43ms/step - loss: 1.1161e-05 - val_loss: 3.4246e-06 - lr: 8.0000e-06\n",
            "Epoch 19/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.1169e-05 - val_loss: 3.3565e-06 - lr: 8.0000e-06\n",
            "Epoch 20/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.1025e-05 - val_loss: 3.2021e-06 - lr: 8.0000e-06\n",
            "Epoch 21/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.1038e-05 - val_loss: 3.3433e-06 - lr: 8.0000e-06\n",
            "Epoch 22/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.0909e-05 - val_loss: 3.1875e-06 - lr: 1.6000e-06\n",
            "Epoch 23/500\n",
            "2902/2902 [==============================] - 130s 45ms/step - loss: 1.0939e-05 - val_loss: 3.2356e-06 - lr: 1.6000e-06\n",
            "Epoch 24/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.0711e-05 - val_loss: 3.1787e-06 - lr: 1.6000e-06\n",
            "Epoch 25/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.0779e-05 - val_loss: 3.1867e-06 - lr: 1.6000e-06\n",
            "Epoch 26/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.0989e-05 - val_loss: 3.1842e-06 - lr: 1.6000e-06\n",
            "Epoch 27/500\n",
            "2902/2902 [==============================] - 129s 45ms/step - loss: 1.0882e-05 - val_loss: 3.1714e-06 - lr: 3.2000e-07\n",
            "Epoch 28/500\n",
            "2902/2902 [==============================] - 129s 45ms/step - loss: 1.1010e-05 - val_loss: 3.1734e-06 - lr: 3.2000e-07\n",
            "Epoch 29/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.0957e-05 - val_loss: 3.1675e-06 - lr: 3.2000e-07\n",
            "Epoch 30/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.0821e-05 - val_loss: 3.1652e-06 - lr: 3.2000e-07\n",
            "Epoch 31/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 1.0956e-05 - val_loss: 3.1771e-06 - lr: 3.2000e-07\n",
            "Epoch 32/500\n",
            "2902/2902 [==============================] - 135s 47ms/step - loss: 1.0891e-05 - val_loss: 3.1685e-06 - lr: 6.4000e-08\n",
            "Epoch 33/500\n",
            "2902/2902 [==============================] - 136s 47ms/step - loss: 1.0901e-05 - val_loss: 3.1664e-06 - lr: 6.4000e-08\n",
            "Epoch 34/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 1.0912e-05 - val_loss: 3.1712e-06 - lr: 6.4000e-08\n",
            "Epoch 35/500\n",
            "2902/2902 [==============================] - 136s 47ms/step - loss: 1.0874e-05 - val_loss: 3.1668e-06 - lr: 6.4000e-08\n",
            "Epoch 36/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.0868e-05 - val_loss: 3.1683e-06 - lr: 6.4000e-08\n",
            "Epoch 37/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.0834e-05 - val_loss: 3.1663e-06 - lr: 1.2800e-08\n",
            "Epoch 38/500\n",
            "2902/2902 [==============================] - 132s 46ms/step - loss: 1.0963e-05 - val_loss: 3.1676e-06 - lr: 1.2800e-08\n",
            "Epoch 39/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.0839e-05 - val_loss: 3.1675e-06 - lr: 1.2800e-08\n",
            "Epoch 40/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.0929e-05 - val_loss: 3.1674e-06 - lr: 1.2800e-08\n",
            "Epoch 41/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.0889e-05 - val_loss: 3.1672e-06 - lr: 1.2800e-08\n",
            "Epoch 42/500\n",
            "2902/2902 [==============================] - 135s 46ms/step - loss: 1.0927e-05 - val_loss: 3.1667e-06 - lr: 1.0000e-08\n",
            "Epoch 43/500\n",
            "2902/2902 [==============================] - 130s 45ms/step - loss: 1.0833e-05 - val_loss: 3.1668e-06 - lr: 1.0000e-08\n",
            "Epoch 44/500\n",
            "2902/2902 [==============================] - 130s 45ms/step - loss: 1.0857e-05 - val_loss: 3.1659e-06 - lr: 1.0000e-08\n",
            "Epoch 45/500\n",
            "2902/2902 [==============================] - 129s 45ms/step - loss: 1.0853e-05 - val_loss: 3.1670e-06 - lr: 1.0000e-08\n",
            "5803/5803 [==============================] - 91s 16ms/step\n",
            "2487/2487 [==============================] - 39s 16ms/step\n",
            "\n",
            "                     model  lstm_layer dense_layer  dropout  batch_size  \\\n",
            "0  256same_model_opt_lstm5  (256, 256)  (256, 256)     0.15          64   \n",
            "\n",
            "   train_RMSE  test_RMSE  \n",
            "0    1.384736   1.385964  \n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_30 (LSTM)              (None, 144, 256)          272384    \n",
            "                                                                 \n",
            " lstm_31 (LSTM)              (None, 144, 256)          525312    \n",
            "                                                                 \n",
            " lstm_32 (LSTM)              (None, 256)               525312    \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_20 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_41 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_21 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_42 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_43 (Dense)            (None, 4)                 1028      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,521,412\n",
            "Trainable params: 1,521,412\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/500\n",
            "2902/2902 [==============================] - 130s 44ms/step - loss: 1.3844e-04 - val_loss: 6.5486e-05 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "2902/2902 [==============================] - 127s 44ms/step - loss: 6.1957e-05 - val_loss: 1.6096e-05 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 4.3141e-05 - val_loss: 3.7698e-05 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "2902/2902 [==============================] - 129s 45ms/step - loss: 3.5749e-05 - val_loss: 1.1259e-05 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 3.3192e-05 - val_loss: 1.8694e-05 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "2902/2902 [==============================] - 132s 45ms/step - loss: 3.3733e-05 - val_loss: 1.4007e-05 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "2902/2902 [==============================] - 132s 45ms/step - loss: 1.5712e-05 - val_loss: 9.4042e-06 - lr: 2.0000e-04\n",
            "Epoch 8/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.5002e-05 - val_loss: 1.7836e-05 - lr: 2.0000e-04\n",
            "Epoch 9/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.4537e-05 - val_loss: 6.5139e-06 - lr: 2.0000e-04\n",
            "Epoch 10/500\n",
            "2902/2902 [==============================] - 132s 46ms/step - loss: 1.4347e-05 - val_loss: 7.0325e-06 - lr: 2.0000e-04\n",
            "Epoch 11/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.3836e-05 - val_loss: 6.4404e-06 - lr: 2.0000e-04\n",
            "Epoch 12/500\n",
            "2902/2902 [==============================] - 132s 46ms/step - loss: 1.1918e-05 - val_loss: 3.8115e-06 - lr: 4.0000e-05\n",
            "Epoch 13/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.1811e-05 - val_loss: 3.7109e-06 - lr: 4.0000e-05\n",
            "Epoch 14/500\n",
            "2902/2902 [==============================] - 132s 46ms/step - loss: 1.1676e-05 - val_loss: 4.4114e-06 - lr: 4.0000e-05\n",
            "Epoch 15/500\n",
            "2902/2902 [==============================] - 132s 45ms/step - loss: 1.1563e-05 - val_loss: 3.8137e-06 - lr: 4.0000e-05\n",
            "Epoch 16/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.1364e-05 - val_loss: 3.6467e-06 - lr: 4.0000e-05\n",
            "Epoch 17/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.1020e-05 - val_loss: 3.2677e-06 - lr: 8.0000e-06\n",
            "Epoch 18/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.0951e-05 - val_loss: 3.2417e-06 - lr: 8.0000e-06\n",
            "Epoch 19/500\n",
            "2902/2902 [==============================] - 129s 44ms/step - loss: 1.1004e-05 - val_loss: 3.2319e-06 - lr: 8.0000e-06\n",
            "Epoch 20/500\n",
            "2902/2902 [==============================] - 130s 45ms/step - loss: 1.0925e-05 - val_loss: 3.1853e-06 - lr: 8.0000e-06\n",
            "Epoch 21/500\n",
            "2902/2902 [==============================] - 129s 45ms/step - loss: 1.0948e-05 - val_loss: 3.2398e-06 - lr: 8.0000e-06\n",
            "Epoch 22/500\n",
            "2902/2902 [==============================] - 142s 49ms/step - loss: 1.0775e-05 - val_loss: 3.1445e-06 - lr: 1.6000e-06\n",
            "Epoch 23/500\n",
            "2902/2902 [==============================] - 145s 50ms/step - loss: 1.0724e-05 - val_loss: 3.1386e-06 - lr: 1.6000e-06\n",
            "Epoch 24/500\n",
            "2902/2902 [==============================] - 144s 50ms/step - loss: 1.0940e-05 - val_loss: 3.1418e-06 - lr: 1.6000e-06\n",
            "Epoch 25/500\n",
            "2902/2902 [==============================] - 148s 51ms/step - loss: 1.0827e-05 - val_loss: 3.1589e-06 - lr: 1.6000e-06\n",
            "Epoch 26/500\n",
            "2902/2902 [==============================] - 149s 51ms/step - loss: 1.0790e-05 - val_loss: 3.1392e-06 - lr: 1.6000e-06\n",
            "Epoch 27/500\n",
            "2902/2902 [==============================] - 150s 52ms/step - loss: 1.0714e-05 - val_loss: 3.1420e-06 - lr: 3.2000e-07\n",
            "Epoch 28/500\n",
            "2902/2902 [==============================] - 151s 52ms/step - loss: 1.0863e-05 - val_loss: 3.1357e-06 - lr: 3.2000e-07\n",
            "Epoch 29/500\n",
            "2902/2902 [==============================] - 152s 53ms/step - loss: 1.0806e-05 - val_loss: 3.1352e-06 - lr: 3.2000e-07\n",
            "Epoch 30/500\n",
            "2902/2902 [==============================] - 148s 51ms/step - loss: 1.0689e-05 - val_loss: 3.1309e-06 - lr: 3.2000e-07\n",
            "Epoch 31/500\n",
            "2902/2902 [==============================] - 149s 51ms/step - loss: 1.0776e-05 - val_loss: 3.1367e-06 - lr: 3.2000e-07\n",
            "Epoch 32/500\n",
            "2902/2902 [==============================] - 145s 50ms/step - loss: 1.0722e-05 - val_loss: 3.1385e-06 - lr: 6.4000e-08\n",
            "Epoch 33/500\n",
            "2902/2902 [==============================] - 142s 49ms/step - loss: 1.0739e-05 - val_loss: 3.1354e-06 - lr: 6.4000e-08\n",
            "Epoch 34/500\n",
            "2902/2902 [==============================] - 145s 50ms/step - loss: 1.0936e-05 - val_loss: 3.1348e-06 - lr: 6.4000e-08\n",
            "Epoch 35/500\n",
            "2902/2902 [==============================] - 141s 49ms/step - loss: 1.0739e-05 - val_loss: 3.1334e-06 - lr: 6.4000e-08\n",
            "Epoch 36/500\n",
            "2902/2902 [==============================] - 146s 50ms/step - loss: 1.0820e-05 - val_loss: 3.1317e-06 - lr: 6.4000e-08\n",
            "Epoch 37/500\n",
            "2902/2902 [==============================] - 147s 51ms/step - loss: 1.0721e-05 - val_loss: 3.1307e-06 - lr: 1.2800e-08\n",
            "Epoch 38/500\n",
            "2902/2902 [==============================] - 145s 50ms/step - loss: 1.0820e-05 - val_loss: 3.1304e-06 - lr: 1.2800e-08\n",
            "Epoch 39/500\n",
            "2902/2902 [==============================] - 145s 50ms/step - loss: 1.0791e-05 - val_loss: 3.1302e-06 - lr: 1.2800e-08\n",
            "Epoch 40/500\n",
            "2902/2902 [==============================] - 151s 52ms/step - loss: 1.0807e-05 - val_loss: 3.1307e-06 - lr: 1.2800e-08\n",
            "Epoch 41/500\n",
            "2902/2902 [==============================] - 145s 50ms/step - loss: 1.0884e-05 - val_loss: 3.1304e-06 - lr: 1.2800e-08\n",
            "Epoch 42/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 1.0882e-05 - val_loss: 3.1302e-06 - lr: 1.0000e-08\n",
            "Epoch 43/500\n",
            "2902/2902 [==============================] - 135s 47ms/step - loss: 1.0743e-05 - val_loss: 3.1300e-06 - lr: 1.0000e-08\n",
            "Epoch 44/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.0766e-05 - val_loss: 3.1298e-06 - lr: 1.0000e-08\n",
            "Epoch 45/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 1.0779e-05 - val_loss: 3.1302e-06 - lr: 1.0000e-08\n",
            "Epoch 46/500\n",
            "2902/2902 [==============================] - 135s 47ms/step - loss: 1.0747e-05 - val_loss: 3.1303e-06 - lr: 1.0000e-08\n",
            "Epoch 47/500\n",
            "2902/2902 [==============================] - 136s 47ms/step - loss: 1.0892e-05 - val_loss: 3.1305e-06 - lr: 1.0000e-08\n",
            "Epoch 48/500\n",
            "2902/2902 [==============================] - 135s 47ms/step - loss: 1.0739e-05 - val_loss: 3.1306e-06 - lr: 1.0000e-08\n",
            "Epoch 49/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.0778e-05 - val_loss: 3.1305e-06 - lr: 1.0000e-08\n",
            "Epoch 50/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.0744e-05 - val_loss: 3.1308e-06 - lr: 1.0000e-08\n",
            "Epoch 51/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.0816e-05 - val_loss: 3.1308e-06 - lr: 1.0000e-08\n",
            "Epoch 52/500\n",
            "2902/2902 [==============================] - 136s 47ms/step - loss: 1.0880e-05 - val_loss: 3.1309e-06 - lr: 1.0000e-08\n",
            "Epoch 53/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.0813e-05 - val_loss: 3.1307e-06 - lr: 1.0000e-08\n",
            "Epoch 54/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.0831e-05 - val_loss: 3.1305e-06 - lr: 1.0000e-08\n",
            "Epoch 55/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.0886e-05 - val_loss: 3.1304e-06 - lr: 1.0000e-08\n",
            "Epoch 56/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.0950e-05 - val_loss: 3.1302e-06 - lr: 1.0000e-08\n",
            "Epoch 57/500\n",
            "2902/2902 [==============================] - 136s 47ms/step - loss: 1.0894e-05 - val_loss: 3.1303e-06 - lr: 1.0000e-08\n",
            "Epoch 58/500\n",
            "2902/2902 [==============================] - 135s 47ms/step - loss: 1.0719e-05 - val_loss: 3.1302e-06 - lr: 1.0000e-08\n",
            "Epoch 59/500\n",
            "2902/2902 [==============================] - 138s 47ms/step - loss: 1.0682e-05 - val_loss: 3.1300e-06 - lr: 1.0000e-08\n",
            "5803/5803 [==============================] - 96s 16ms/step\n",
            "2487/2487 [==============================] - 42s 17ms/step\n",
            "\n",
            "                     model  lstm_layer dense_layer  dropout  batch_size  \\\n",
            "0  256same_model_opt_lstm6  (256, 256)  (256, 256)     0.15          64   \n",
            "\n",
            "   train_RMSE  test_RMSE  \n",
            "0     1.33922   1.357082  \n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_33 (LSTM)              (None, 144, 256)          272384    \n",
            "                                                                 \n",
            " lstm_34 (LSTM)              (None, 144, 256)          525312    \n",
            "                                                                 \n",
            " lstm_35 (LSTM)              (None, 256)               525312    \n",
            "                                                                 \n",
            " dense_44 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_22 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_45 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_23 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_46 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_47 (Dense)            (None, 4)                 1028      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,521,412\n",
            "Trainable params: 1,521,412\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/500\n",
            "2902/2902 [==============================] - 140s 47ms/step - loss: 1.3339e-04 - val_loss: 1.5798e-04 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 5.3789e-05 - val_loss: 4.0923e-05 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "2902/2902 [==============================] - 135s 47ms/step - loss: 4.2097e-05 - val_loss: 1.3476e-05 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 3.9276e-05 - val_loss: 2.4682e-05 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 3.3831e-05 - val_loss: 1.5502e-05 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "2902/2902 [==============================] - 135s 47ms/step - loss: 3.0675e-05 - val_loss: 1.0995e-05 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "2902/2902 [==============================] - 135s 47ms/step - loss: 2.7385e-05 - val_loss: 2.2154e-05 - lr: 0.0010\n",
            "Epoch 8/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.4787e-05 - val_loss: 5.9152e-06 - lr: 2.0000e-04\n",
            "Epoch 9/500\n",
            "2902/2902 [==============================] - 136s 47ms/step - loss: 1.4300e-05 - val_loss: 9.6227e-06 - lr: 2.0000e-04\n",
            "Epoch 10/500\n",
            "2902/2902 [==============================] - 140s 48ms/step - loss: 1.3883e-05 - val_loss: 4.9207e-06 - lr: 2.0000e-04\n",
            "Epoch 11/500\n",
            "2902/2902 [==============================] - 138s 47ms/step - loss: 1.3846e-05 - val_loss: 9.1384e-06 - lr: 2.0000e-04\n",
            "Epoch 12/500\n",
            "2902/2902 [==============================] - 135s 47ms/step - loss: 1.3535e-05 - val_loss: 4.7383e-06 - lr: 2.0000e-04\n",
            "Epoch 13/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 1.1603e-05 - val_loss: 3.3269e-06 - lr: 4.0000e-05\n",
            "Epoch 14/500\n",
            "2902/2902 [==============================] - 138s 48ms/step - loss: 1.1527e-05 - val_loss: 3.7073e-06 - lr: 4.0000e-05\n",
            "Epoch 15/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.1467e-05 - val_loss: 3.2313e-06 - lr: 4.0000e-05\n",
            "Epoch 16/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.1226e-05 - val_loss: 3.3625e-06 - lr: 4.0000e-05\n",
            "Epoch 17/500\n",
            "2902/2902 [==============================] - 138s 47ms/step - loss: 1.1173e-05 - val_loss: 3.3078e-06 - lr: 4.0000e-05\n",
            "Epoch 18/500\n",
            "2902/2902 [==============================] - 135s 46ms/step - loss: 1.0697e-05 - val_loss: 3.0916e-06 - lr: 8.0000e-06\n",
            "Epoch 19/500\n",
            "2902/2902 [==============================] - 132s 46ms/step - loss: 1.0797e-05 - val_loss: 3.1035e-06 - lr: 8.0000e-06\n",
            "Epoch 20/500\n",
            "2902/2902 [==============================] - 138s 48ms/step - loss: 1.0790e-05 - val_loss: 3.1614e-06 - lr: 8.0000e-06\n",
            "Epoch 21/500\n",
            "2902/2902 [==============================] - 139s 48ms/step - loss: 1.0746e-05 - val_loss: 3.1445e-06 - lr: 8.0000e-06\n",
            "Epoch 22/500\n",
            "2902/2902 [==============================] - 138s 47ms/step - loss: 1.0641e-05 - val_loss: 3.1697e-06 - lr: 8.0000e-06\n",
            "Epoch 23/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 1.0574e-05 - val_loss: 3.0815e-06 - lr: 1.6000e-06\n",
            "Epoch 24/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 1.0554e-05 - val_loss: 3.0910e-06 - lr: 1.6000e-06\n",
            "Epoch 25/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.0544e-05 - val_loss: 3.0729e-06 - lr: 1.6000e-06\n",
            "Epoch 26/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 1.0480e-05 - val_loss: 3.0782e-06 - lr: 1.6000e-06\n",
            "Epoch 27/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 1.0565e-05 - val_loss: 3.0906e-06 - lr: 1.6000e-06\n",
            "Epoch 28/500\n",
            "2902/2902 [==============================] - 138s 47ms/step - loss: 1.0605e-05 - val_loss: 3.0687e-06 - lr: 3.2000e-07\n",
            "Epoch 29/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.0612e-05 - val_loss: 3.0693e-06 - lr: 3.2000e-07\n",
            "Epoch 30/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.0677e-05 - val_loss: 3.0682e-06 - lr: 3.2000e-07\n",
            "Epoch 31/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.0557e-05 - val_loss: 3.0699e-06 - lr: 3.2000e-07\n",
            "Epoch 32/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 1.0385e-05 - val_loss: 3.0779e-06 - lr: 3.2000e-07\n",
            "Epoch 33/500\n",
            "2902/2902 [==============================] - 132s 45ms/step - loss: 1.0647e-05 - val_loss: 3.0738e-06 - lr: 6.4000e-08\n",
            "Epoch 34/500\n",
            "2902/2902 [==============================] - 132s 46ms/step - loss: 1.0570e-05 - val_loss: 3.0718e-06 - lr: 6.4000e-08\n",
            "Epoch 35/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.0551e-05 - val_loss: 3.0722e-06 - lr: 6.4000e-08\n",
            "Epoch 36/500\n",
            "2902/2902 [==============================] - 135s 46ms/step - loss: 1.0507e-05 - val_loss: 3.0729e-06 - lr: 6.4000e-08\n",
            "Epoch 37/500\n",
            "2902/2902 [==============================] - 138s 47ms/step - loss: 1.0562e-05 - val_loss: 3.0720e-06 - lr: 6.4000e-08\n",
            "Epoch 38/500\n",
            "2902/2902 [==============================] - 132s 46ms/step - loss: 1.0619e-05 - val_loss: 3.0709e-06 - lr: 1.2800e-08\n",
            "Epoch 39/500\n",
            "2902/2902 [==============================] - 136s 47ms/step - loss: 1.0567e-05 - val_loss: 3.0704e-06 - lr: 1.2800e-08\n",
            "Epoch 40/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.0455e-05 - val_loss: 3.0703e-06 - lr: 1.2800e-08\n",
            "Epoch 41/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.0599e-05 - val_loss: 3.0704e-06 - lr: 1.2800e-08\n",
            "Epoch 42/500\n",
            "2902/2902 [==============================] - 132s 45ms/step - loss: 1.0520e-05 - val_loss: 3.0699e-06 - lr: 1.2800e-08\n",
            "Epoch 43/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.0538e-05 - val_loss: 3.0701e-06 - lr: 1.0000e-08\n",
            "Epoch 44/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.0490e-05 - val_loss: 3.0705e-06 - lr: 1.0000e-08\n",
            "Epoch 45/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.0581e-05 - val_loss: 3.0703e-06 - lr: 1.0000e-08\n",
            "5803/5803 [==============================] - 92s 16ms/step\n",
            "2487/2487 [==============================] - 40s 16ms/step\n",
            "\n",
            "                     model  lstm_layer dense_layer  dropout  batch_size  \\\n",
            "0  256same_model_opt_lstm7  (256, 256)  (256, 256)     0.15          64   \n",
            "\n",
            "   train_RMSE  test_RMSE  \n",
            "0    1.311752   1.326555  \n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_36 (LSTM)              (None, 144, 256)          272384    \n",
            "                                                                 \n",
            " lstm_37 (LSTM)              (None, 144, 256)          525312    \n",
            "                                                                 \n",
            " lstm_38 (LSTM)              (None, 256)               525312    \n",
            "                                                                 \n",
            " dense_48 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_24 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_49 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_25 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_50 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_51 (Dense)            (None, 4)                 1028      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,521,412\n",
            "Trainable params: 1,521,412\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/500\n",
            "2902/2902 [==============================] - 135s 46ms/step - loss: 1.2333e-04 - val_loss: 3.9548e-05 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "2902/2902 [==============================] - 135s 47ms/step - loss: 5.2492e-05 - val_loss: 1.9716e-05 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "2902/2902 [==============================] - 139s 48ms/step - loss: 4.1981e-05 - val_loss: 1.7708e-05 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "2902/2902 [==============================] - 135s 46ms/step - loss: 3.8744e-05 - val_loss: 2.5338e-05 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 3.1712e-05 - val_loss: 2.5868e-05 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 3.0527e-05 - val_loss: 9.7498e-06 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "2902/2902 [==============================] - 135s 47ms/step - loss: 1.5558e-05 - val_loss: 9.2537e-06 - lr: 2.0000e-04\n",
            "Epoch 8/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.5117e-05 - val_loss: 5.0349e-06 - lr: 2.0000e-04\n",
            "Epoch 9/500\n",
            "2902/2902 [==============================] - 136s 47ms/step - loss: 1.4337e-05 - val_loss: 6.9577e-06 - lr: 2.0000e-04\n",
            "Epoch 10/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.4726e-05 - val_loss: 6.8706e-06 - lr: 2.0000e-04\n",
            "Epoch 11/500\n",
            "2902/2902 [==============================] - 130s 45ms/step - loss: 1.4112e-05 - val_loss: 5.7543e-06 - lr: 2.0000e-04\n",
            "Epoch 12/500\n",
            "2902/2902 [==============================] - 129s 45ms/step - loss: 1.2121e-05 - val_loss: 3.6140e-06 - lr: 4.0000e-05\n",
            "Epoch 13/500\n",
            "2902/2902 [==============================] - 128s 44ms/step - loss: 1.1976e-05 - val_loss: 4.6051e-06 - lr: 4.0000e-05\n",
            "Epoch 14/500\n",
            "2902/2902 [==============================] - 129s 44ms/step - loss: 1.1717e-05 - val_loss: 3.4873e-06 - lr: 4.0000e-05\n",
            "Epoch 15/500\n",
            "2902/2902 [==============================] - 130s 45ms/step - loss: 1.1695e-05 - val_loss: 3.6117e-06 - lr: 4.0000e-05\n",
            "Epoch 16/500\n",
            "2902/2902 [==============================] - 128s 44ms/step - loss: 1.1842e-05 - val_loss: 3.5172e-06 - lr: 4.0000e-05\n",
            "Epoch 17/500\n",
            "2902/2902 [==============================] - 130s 45ms/step - loss: 1.1362e-05 - val_loss: 3.3539e-06 - lr: 8.0000e-06\n",
            "Epoch 18/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.1171e-05 - val_loss: 3.4716e-06 - lr: 8.0000e-06\n",
            "Epoch 19/500\n",
            "2902/2902 [==============================] - 132s 45ms/step - loss: 1.1244e-05 - val_loss: 3.3943e-06 - lr: 8.0000e-06\n",
            "Epoch 20/500\n",
            "2902/2902 [==============================] - 132s 46ms/step - loss: 1.1052e-05 - val_loss: 3.3609e-06 - lr: 8.0000e-06\n",
            "Epoch 21/500\n",
            "2902/2902 [==============================] - 132s 46ms/step - loss: 1.1146e-05 - val_loss: 3.4529e-06 - lr: 8.0000e-06\n",
            "Epoch 22/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.1044e-05 - val_loss: 3.3046e-06 - lr: 1.6000e-06\n",
            "Epoch 23/500\n",
            "2902/2902 [==============================] - 129s 44ms/step - loss: 1.0993e-05 - val_loss: 3.2996e-06 - lr: 1.6000e-06\n",
            "Epoch 24/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.0954e-05 - val_loss: 3.2997e-06 - lr: 1.6000e-06\n",
            "Epoch 25/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.0993e-05 - val_loss: 3.3202e-06 - lr: 1.6000e-06\n",
            "Epoch 26/500\n",
            "2902/2902 [==============================] - 130s 45ms/step - loss: 1.1025e-05 - val_loss: 3.2923e-06 - lr: 1.6000e-06\n",
            "Epoch 27/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.1010e-05 - val_loss: 3.2885e-06 - lr: 3.2000e-07\n",
            "Epoch 28/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.0972e-05 - val_loss: 3.2875e-06 - lr: 3.2000e-07\n",
            "Epoch 29/500\n",
            "2902/2902 [==============================] - 130s 45ms/step - loss: 1.0969e-05 - val_loss: 3.2890e-06 - lr: 3.2000e-07\n",
            "Epoch 30/500\n",
            "2902/2902 [==============================] - 130s 45ms/step - loss: 1.0931e-05 - val_loss: 3.2828e-06 - lr: 3.2000e-07\n",
            "Epoch 31/500\n",
            "2902/2902 [==============================] - 129s 45ms/step - loss: 1.0957e-05 - val_loss: 3.2875e-06 - lr: 3.2000e-07\n",
            "Epoch 32/500\n",
            "2902/2902 [==============================] - 130s 45ms/step - loss: 1.1016e-05 - val_loss: 3.2839e-06 - lr: 6.4000e-08\n",
            "Epoch 33/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.0981e-05 - val_loss: 3.2828e-06 - lr: 6.4000e-08\n",
            "Epoch 34/500\n",
            "2902/2902 [==============================] - 132s 45ms/step - loss: 1.1059e-05 - val_loss: 3.2839e-06 - lr: 6.4000e-08\n",
            "Epoch 35/500\n",
            "2902/2902 [==============================] - 130s 45ms/step - loss: 1.0912e-05 - val_loss: 3.2829e-06 - lr: 6.4000e-08\n",
            "Epoch 36/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.1002e-05 - val_loss: 3.2850e-06 - lr: 6.4000e-08\n",
            "Epoch 37/500\n",
            "2902/2902 [==============================] - 129s 44ms/step - loss: 1.0934e-05 - val_loss: 3.2852e-06 - lr: 1.2800e-08\n",
            "Epoch 38/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.0903e-05 - val_loss: 3.2847e-06 - lr: 1.2800e-08\n",
            "Epoch 39/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.0979e-05 - val_loss: 3.2843e-06 - lr: 1.2800e-08\n",
            "Epoch 40/500\n",
            "2902/2902 [==============================] - 130s 45ms/step - loss: 1.0915e-05 - val_loss: 3.2841e-06 - lr: 1.2800e-08\n",
            "Epoch 41/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.1062e-05 - val_loss: 3.2839e-06 - lr: 1.2800e-08\n",
            "Epoch 42/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.1013e-05 - val_loss: 3.2836e-06 - lr: 1.0000e-08\n",
            "Epoch 43/500\n",
            "2902/2902 [==============================] - 135s 46ms/step - loss: 1.0975e-05 - val_loss: 3.2838e-06 - lr: 1.0000e-08\n",
            "Epoch 44/500\n",
            "2902/2902 [==============================] - 136s 47ms/step - loss: 1.0961e-05 - val_loss: 3.2832e-06 - lr: 1.0000e-08\n",
            "Epoch 45/500\n",
            "2902/2902 [==============================] - 135s 47ms/step - loss: 1.0910e-05 - val_loss: 3.2835e-06 - lr: 1.0000e-08\n",
            "Epoch 46/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.0995e-05 - val_loss: 3.2835e-06 - lr: 1.0000e-08\n",
            "Epoch 47/500\n",
            "2902/2902 [==============================] - 130s 45ms/step - loss: 1.1098e-05 - val_loss: 3.2832e-06 - lr: 1.0000e-08\n",
            "Epoch 48/500\n",
            "2902/2902 [==============================] - 130s 45ms/step - loss: 1.0912e-05 - val_loss: 3.2831e-06 - lr: 1.0000e-08\n",
            "5803/5803 [==============================] - 94s 16ms/step\n",
            "2487/2487 [==============================] - 39s 16ms/step\n",
            "\n",
            "                     model  lstm_layer dense_layer  dropout  batch_size  \\\n",
            "0  256same_model_opt_lstm8  (256, 256)  (256, 256)     0.15          64   \n",
            "\n",
            "   train_RMSE  test_RMSE  \n",
            "0     1.33445   1.383197  \n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_39 (LSTM)              (None, 144, 256)          272384    \n",
            "                                                                 \n",
            " lstm_40 (LSTM)              (None, 144, 256)          525312    \n",
            "                                                                 \n",
            " lstm_41 (LSTM)              (None, 256)               525312    \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_26 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_27 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_54 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_55 (Dense)            (None, 4)                 1028      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,521,412\n",
            "Trainable params: 1,521,412\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/500\n",
            "2902/2902 [==============================] - 140s 47ms/step - loss: 1.8397e-04 - val_loss: 2.8257e-05 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 6.0625e-05 - val_loss: 3.4942e-05 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "2902/2902 [==============================] - 135s 46ms/step - loss: 4.3188e-05 - val_loss: 3.3630e-05 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 3.8658e-05 - val_loss: 1.2184e-05 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "2902/2902 [==============================] - 138s 48ms/step - loss: 3.5541e-05 - val_loss: 7.0682e-05 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "2902/2902 [==============================] - 138s 47ms/step - loss: 2.9209e-05 - val_loss: 1.6728e-05 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "2902/2902 [==============================] - 138s 47ms/step - loss: 1.5237e-05 - val_loss: 4.9690e-06 - lr: 2.0000e-04\n",
            "Epoch 8/500\n",
            "2902/2902 [==============================] - 136s 47ms/step - loss: 1.4774e-05 - val_loss: 6.8205e-06 - lr: 2.0000e-04\n",
            "Epoch 9/500\n",
            "2902/2902 [==============================] - 135s 47ms/step - loss: 1.4570e-05 - val_loss: 6.9181e-06 - lr: 2.0000e-04\n",
            "Epoch 10/500\n",
            "2902/2902 [==============================] - 132s 46ms/step - loss: 1.4343e-05 - val_loss: 4.6595e-06 - lr: 2.0000e-04\n",
            "Epoch 11/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.4042e-05 - val_loss: 3.9934e-06 - lr: 2.0000e-04\n",
            "Epoch 12/500\n",
            "2902/2902 [==============================] - 129s 45ms/step - loss: 1.2024e-05 - val_loss: 3.4352e-06 - lr: 4.0000e-05\n",
            "Epoch 13/500\n",
            "2902/2902 [==============================] - 132s 46ms/step - loss: 1.1785e-05 - val_loss: 3.5043e-06 - lr: 4.0000e-05\n",
            "Epoch 14/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.1572e-05 - val_loss: 3.7536e-06 - lr: 4.0000e-05\n",
            "Epoch 15/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.1764e-05 - val_loss: 4.1135e-06 - lr: 4.0000e-05\n",
            "Epoch 16/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.1551e-05 - val_loss: 3.3856e-06 - lr: 4.0000e-05\n",
            "Epoch 17/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.1130e-05 - val_loss: 3.2300e-06 - lr: 8.0000e-06\n",
            "Epoch 18/500\n",
            "2902/2902 [==============================] - 132s 46ms/step - loss: 1.1007e-05 - val_loss: 3.1528e-06 - lr: 8.0000e-06\n",
            "Epoch 19/500\n",
            "2902/2902 [==============================] - 132s 45ms/step - loss: 1.0999e-05 - val_loss: 3.2540e-06 - lr: 8.0000e-06\n",
            "Epoch 20/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.1013e-05 - val_loss: 3.2033e-06 - lr: 8.0000e-06\n",
            "Epoch 21/500\n",
            "2902/2902 [==============================] - 136s 47ms/step - loss: 1.0978e-05 - val_loss: 3.2794e-06 - lr: 8.0000e-06\n",
            "Epoch 22/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.0872e-05 - val_loss: 3.1748e-06 - lr: 1.6000e-06\n",
            "Epoch 23/500\n",
            "2902/2902 [==============================] - 139s 48ms/step - loss: 1.0738e-05 - val_loss: 3.1498e-06 - lr: 1.6000e-06\n",
            "Epoch 24/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 1.0895e-05 - val_loss: 3.1483e-06 - lr: 1.6000e-06\n",
            "Epoch 25/500\n",
            "2902/2902 [==============================] - 135s 47ms/step - loss: 1.0742e-05 - val_loss: 3.1359e-06 - lr: 1.6000e-06\n",
            "Epoch 26/500\n",
            "2902/2902 [==============================] - 135s 47ms/step - loss: 1.0780e-05 - val_loss: 3.1706e-06 - lr: 1.6000e-06\n",
            "Epoch 27/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.0869e-05 - val_loss: 3.1380e-06 - lr: 3.2000e-07\n",
            "Epoch 28/500\n",
            "2902/2902 [==============================] - 136s 47ms/step - loss: 1.0763e-05 - val_loss: 3.1303e-06 - lr: 3.2000e-07\n",
            "Epoch 29/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.0797e-05 - val_loss: 3.1417e-06 - lr: 3.2000e-07\n",
            "Epoch 30/500\n",
            "2902/2902 [==============================] - 136s 47ms/step - loss: 1.0871e-05 - val_loss: 3.1375e-06 - lr: 3.2000e-07\n",
            "Epoch 31/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.0839e-05 - val_loss: 3.1328e-06 - lr: 3.2000e-07\n",
            "Epoch 32/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.0755e-05 - val_loss: 3.1296e-06 - lr: 6.4000e-08\n",
            "Epoch 33/500\n",
            "2902/2902 [==============================] - 131s 45ms/step - loss: 1.0773e-05 - val_loss: 3.1278e-06 - lr: 6.4000e-08\n",
            "Epoch 34/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.0779e-05 - val_loss: 3.1262e-06 - lr: 6.4000e-08\n",
            "Epoch 35/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 1.0769e-05 - val_loss: 3.1286e-06 - lr: 6.4000e-08\n",
            "Epoch 36/500\n",
            "2902/2902 [==============================] - 145s 50ms/step - loss: 1.0780e-05 - val_loss: 3.1300e-06 - lr: 6.4000e-08\n",
            "Epoch 37/500\n",
            "2902/2902 [==============================] - 145s 50ms/step - loss: 1.0694e-05 - val_loss: 3.1284e-06 - lr: 1.2800e-08\n",
            "Epoch 38/500\n",
            "2902/2902 [==============================] - 145s 50ms/step - loss: 1.0769e-05 - val_loss: 3.1279e-06 - lr: 1.2800e-08\n",
            "Epoch 39/500\n",
            "2902/2902 [==============================] - 145s 50ms/step - loss: 1.0675e-05 - val_loss: 3.1276e-06 - lr: 1.2800e-08\n",
            "Epoch 40/500\n",
            "2902/2902 [==============================] - 146s 50ms/step - loss: 1.0857e-05 - val_loss: 3.1277e-06 - lr: 1.2800e-08\n",
            "Epoch 41/500\n",
            "2902/2902 [==============================] - 145s 50ms/step - loss: 1.0808e-05 - val_loss: 3.1275e-06 - lr: 1.2800e-08\n",
            "Epoch 42/500\n",
            "2902/2902 [==============================] - 144s 49ms/step - loss: 1.0752e-05 - val_loss: 3.1274e-06 - lr: 1.0000e-08\n",
            "Epoch 43/500\n",
            "2902/2902 [==============================] - 144s 50ms/step - loss: 1.0807e-05 - val_loss: 3.1274e-06 - lr: 1.0000e-08\n",
            "Epoch 44/500\n",
            "2902/2902 [==============================] - 145s 50ms/step - loss: 1.0774e-05 - val_loss: 3.1272e-06 - lr: 1.0000e-08\n",
            "Epoch 45/500\n",
            "2902/2902 [==============================] - 145s 50ms/step - loss: 1.0828e-05 - val_loss: 3.1279e-06 - lr: 1.0000e-08\n",
            "Epoch 46/500\n",
            "2902/2902 [==============================] - 141s 48ms/step - loss: 1.0834e-05 - val_loss: 3.1279e-06 - lr: 1.0000e-08\n",
            "Epoch 47/500\n",
            "2902/2902 [==============================] - 142s 49ms/step - loss: 1.0788e-05 - val_loss: 3.1277e-06 - lr: 1.0000e-08\n",
            "Epoch 48/500\n",
            "2902/2902 [==============================] - 140s 48ms/step - loss: 1.0725e-05 - val_loss: 3.1272e-06 - lr: 1.0000e-08\n",
            "Epoch 49/500\n",
            "2902/2902 [==============================] - 140s 48ms/step - loss: 1.0840e-05 - val_loss: 3.1278e-06 - lr: 1.0000e-08\n",
            "5803/5803 [==============================] - 100s 17ms/step\n",
            "2487/2487 [==============================] - 44s 18ms/step\n",
            "\n",
            "                     model  lstm_layer dense_layer  dropout  batch_size  \\\n",
            "0  256same_model_opt_lstm9  (256, 256)  (256, 256)     0.15          64   \n",
            "\n",
            "   train_RMSE  test_RMSE  \n",
            "0    1.363298   1.372461  \n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_42 (LSTM)              (None, 144, 256)          272384    \n",
            "                                                                 \n",
            " lstm_43 (LSTM)              (None, 144, 256)          525312    \n",
            "                                                                 \n",
            " lstm_44 (LSTM)              (None, 256)               525312    \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_28 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_29 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_59 (Dense)            (None, 4)                 1028      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,521,412\n",
            "Trainable params: 1,521,412\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/500\n",
            "2902/2902 [==============================] - 139s 47ms/step - loss: 1.8415e-04 - val_loss: 3.4724e-05 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "2902/2902 [==============================] - 138s 47ms/step - loss: 5.5641e-05 - val_loss: 1.8579e-05 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "2902/2902 [==============================] - 139s 48ms/step - loss: 4.4338e-05 - val_loss: 2.6501e-05 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 3.9422e-05 - val_loss: 1.3995e-04 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "2902/2902 [==============================] - 135s 46ms/step - loss: 3.6947e-05 - val_loss: 2.6703e-05 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "2902/2902 [==============================] - 139s 48ms/step - loss: 3.2254e-05 - val_loss: 4.4284e-05 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 1.6128e-05 - val_loss: 6.0989e-06 - lr: 2.0000e-04\n",
            "Epoch 8/500\n",
            "2902/2902 [==============================] - 135s 47ms/step - loss: 1.5449e-05 - val_loss: 7.9683e-06 - lr: 2.0000e-04\n",
            "Epoch 9/500\n",
            "2902/2902 [==============================] - 139s 48ms/step - loss: 1.5062e-05 - val_loss: 4.2652e-06 - lr: 2.0000e-04\n",
            "Epoch 10/500\n",
            "2902/2902 [==============================] - 141s 49ms/step - loss: 1.4668e-05 - val_loss: 8.6239e-06 - lr: 2.0000e-04\n",
            "Epoch 11/500\n",
            "2902/2902 [==============================] - 141s 49ms/step - loss: 1.4738e-05 - val_loss: 5.3611e-06 - lr: 2.0000e-04\n",
            "Epoch 12/500\n",
            "2902/2902 [==============================] - 141s 49ms/step - loss: 1.2408e-05 - val_loss: 3.4787e-06 - lr: 4.0000e-05\n",
            "Epoch 13/500\n",
            "2902/2902 [==============================] - 129s 44ms/step - loss: 1.2238e-05 - val_loss: 3.4514e-06 - lr: 4.0000e-05\n",
            "Epoch 14/500\n",
            "2902/2902 [==============================] - 140s 48ms/step - loss: 1.2175e-05 - val_loss: 3.6192e-06 - lr: 4.0000e-05\n",
            "Epoch 15/500\n",
            "2902/2902 [==============================] - 140s 48ms/step - loss: 1.2090e-05 - val_loss: 3.2324e-06 - lr: 4.0000e-05\n",
            "Epoch 16/500\n",
            "2902/2902 [==============================] - 143s 49ms/step - loss: 1.1937e-05 - val_loss: 3.7810e-06 - lr: 4.0000e-05\n",
            "Epoch 17/500\n",
            "2902/2902 [==============================] - 142s 49ms/step - loss: 1.1519e-05 - val_loss: 3.1288e-06 - lr: 8.0000e-06\n",
            "Epoch 18/500\n",
            "2902/2902 [==============================] - 140s 48ms/step - loss: 1.1469e-05 - val_loss: 3.1451e-06 - lr: 8.0000e-06\n",
            "Epoch 19/500\n",
            "2902/2902 [==============================] - 140s 48ms/step - loss: 1.1447e-05 - val_loss: 3.1510e-06 - lr: 8.0000e-06\n",
            "Epoch 20/500\n",
            "2902/2902 [==============================] - 140s 48ms/step - loss: 1.1467e-05 - val_loss: 3.3204e-06 - lr: 8.0000e-06\n",
            "Epoch 21/500\n",
            "2902/2902 [==============================] - 136s 47ms/step - loss: 1.1281e-05 - val_loss: 3.1118e-06 - lr: 8.0000e-06\n",
            "Epoch 22/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 1.1403e-05 - val_loss: 3.1183e-06 - lr: 1.6000e-06\n",
            "Epoch 23/500\n",
            "2902/2902 [==============================] - 136s 47ms/step - loss: 1.1411e-05 - val_loss: 3.0898e-06 - lr: 1.6000e-06\n",
            "Epoch 24/500\n",
            "2902/2902 [==============================] - 135s 46ms/step - loss: 1.1211e-05 - val_loss: 3.0798e-06 - lr: 1.6000e-06\n",
            "Epoch 25/500\n",
            "2902/2902 [==============================] - 141s 49ms/step - loss: 1.1274e-05 - val_loss: 3.1004e-06 - lr: 1.6000e-06\n",
            "Epoch 26/500\n",
            "2902/2902 [==============================] - 138s 47ms/step - loss: 1.1382e-05 - val_loss: 3.1089e-06 - lr: 1.6000e-06\n",
            "Epoch 27/500\n",
            "2902/2902 [==============================] - 132s 46ms/step - loss: 1.1237e-05 - val_loss: 3.0824e-06 - lr: 3.2000e-07\n",
            "Epoch 28/500\n",
            "2902/2902 [==============================] - 138s 47ms/step - loss: 1.1202e-05 - val_loss: 3.0857e-06 - lr: 3.2000e-07\n",
            "Epoch 29/500\n",
            "2902/2902 [==============================] - 139s 48ms/step - loss: 1.1169e-05 - val_loss: 3.0914e-06 - lr: 3.2000e-07\n",
            "Epoch 30/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 1.1276e-05 - val_loss: 3.0827e-06 - lr: 3.2000e-07\n",
            "Epoch 31/500\n",
            "2902/2902 [==============================] - 138s 47ms/step - loss: 1.1168e-05 - val_loss: 3.0764e-06 - lr: 3.2000e-07\n",
            "Epoch 32/500\n",
            "2902/2902 [==============================] - 138s 47ms/step - loss: 1.1280e-05 - val_loss: 3.0720e-06 - lr: 6.4000e-08\n",
            "Epoch 33/500\n",
            "2902/2902 [==============================] - 139s 48ms/step - loss: 1.1238e-05 - val_loss: 3.0737e-06 - lr: 6.4000e-08\n",
            "Epoch 34/500\n",
            "2902/2902 [==============================] - 142s 49ms/step - loss: 1.1278e-05 - val_loss: 3.0738e-06 - lr: 6.4000e-08\n",
            "Epoch 35/500\n",
            "2902/2902 [==============================] - 141s 49ms/step - loss: 1.1301e-05 - val_loss: 3.0726e-06 - lr: 6.4000e-08\n",
            "Epoch 36/500\n",
            "2902/2902 [==============================] - 141s 49ms/step - loss: 1.1190e-05 - val_loss: 3.0766e-06 - lr: 6.4000e-08\n",
            "Epoch 37/500\n",
            "2902/2902 [==============================] - 140s 48ms/step - loss: 1.1229e-05 - val_loss: 3.0744e-06 - lr: 1.2800e-08\n",
            "Epoch 38/500\n",
            "2902/2902 [==============================] - 143s 49ms/step - loss: 1.1265e-05 - val_loss: 3.0736e-06 - lr: 1.2800e-08\n",
            "Epoch 39/500\n",
            "2902/2902 [==============================] - 142s 49ms/step - loss: 1.1216e-05 - val_loss: 3.0729e-06 - lr: 1.2800e-08\n",
            "Epoch 40/500\n",
            "2902/2902 [==============================] - 139s 48ms/step - loss: 1.1172e-05 - val_loss: 3.0729e-06 - lr: 1.2800e-08\n",
            "Epoch 41/500\n",
            "2902/2902 [==============================] - 142s 49ms/step - loss: 1.1352e-05 - val_loss: 3.0734e-06 - lr: 1.2800e-08\n",
            "Epoch 42/500\n",
            "2902/2902 [==============================] - 142s 49ms/step - loss: 1.1248e-05 - val_loss: 3.0733e-06 - lr: 1.0000e-08\n",
            "Epoch 43/500\n",
            "2902/2902 [==============================] - 143s 49ms/step - loss: 1.1312e-05 - val_loss: 3.0732e-06 - lr: 1.0000e-08\n",
            "Epoch 44/500\n",
            "2902/2902 [==============================] - 142s 49ms/step - loss: 1.1295e-05 - val_loss: 3.0736e-06 - lr: 1.0000e-08\n",
            "Epoch 45/500\n",
            "2902/2902 [==============================] - 140s 48ms/step - loss: 1.1238e-05 - val_loss: 3.0731e-06 - lr: 1.0000e-08\n",
            "Epoch 46/500\n",
            "2902/2902 [==============================] - 138s 48ms/step - loss: 1.1107e-05 - val_loss: 3.0734e-06 - lr: 1.0000e-08\n",
            "Epoch 47/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 1.1265e-05 - val_loss: 3.0731e-06 - lr: 1.0000e-08\n",
            "5803/5803 [==============================] - 88s 15ms/step\n",
            "2487/2487 [==============================] - 40s 16ms/step\n",
            "\n",
            "                      model  lstm_layer dense_layer  dropout  batch_size  \\\n",
            "0  256same_model_opt_lstm10  (256, 256)  (256, 256)     0.15          64   \n",
            "\n",
            "   train_RMSE  test_RMSE  \n",
            "0    1.381565   1.361507  \n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_45 (LSTM)              (None, 144, 256)          272384    \n",
            "                                                                 \n",
            " lstm_46 (LSTM)              (None, 144, 256)          525312    \n",
            "                                                                 \n",
            " lstm_47 (LSTM)              (None, 256)               525312    \n",
            "                                                                 \n",
            " dense_60 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_30 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_31 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_63 (Dense)            (None, 4)                 1028      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,521,412\n",
            "Trainable params: 1,521,412\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/500\n",
            "2902/2902 [==============================] - 140s 47ms/step - loss: 1.5531e-04 - val_loss: 5.8300e-05 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 5.5661e-05 - val_loss: 1.7779e-05 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "2902/2902 [==============================] - 128s 44ms/step - loss: 4.6294e-05 - val_loss: 2.6945e-05 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "2902/2902 [==============================] - 132s 46ms/step - loss: 3.5084e-05 - val_loss: 1.7731e-05 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "2902/2902 [==============================] - 129s 45ms/step - loss: 3.3671e-05 - val_loss: 3.8960e-05 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "2902/2902 [==============================] - 133s 46ms/step - loss: 3.0548e-05 - val_loss: 5.7212e-05 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "2902/2902 [==============================] - 135s 46ms/step - loss: 1.5766e-05 - val_loss: 7.4696e-06 - lr: 2.0000e-04\n",
            "Epoch 8/500\n",
            "2902/2902 [==============================] - 138s 48ms/step - loss: 1.5175e-05 - val_loss: 8.2910e-06 - lr: 2.0000e-04\n",
            "Epoch 9/500\n",
            "2902/2902 [==============================] - 134s 46ms/step - loss: 1.4528e-05 - val_loss: 5.3885e-06 - lr: 2.0000e-04\n",
            "Epoch 10/500\n",
            "2902/2902 [==============================] - 137s 47ms/step - loss: 1.4507e-05 - val_loss: 5.1402e-06 - lr: 2.0000e-04\n",
            "Epoch 11/500\n",
            "2902/2902 [==============================] - 138s 48ms/step - loss: 1.4266e-05 - val_loss: 6.3227e-06 - lr: 2.0000e-04\n",
            "Epoch 12/500\n",
            "2902/2902 [==============================] - 143s 49ms/step - loss: 1.2107e-05 - val_loss: 4.2245e-06 - lr: 4.0000e-05\n",
            "Epoch 13/500\n",
            "2475/2902 [========================>.....] - ETA: 18s - loss: 1.2083e-05"
          ]
        }
      ],
      "source": [
        "result_pilename = 'lstm_앙상블용 동일모델.csv'\n",
        "\n",
        "# model setting\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience = 15, restore_best_weights = True)\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr = 0.00000001) # 대체적으로 e-6에서 수렴하니 학습률 e-8까지는 보겠다.\n",
        "\n",
        "for i, params in enumerate(param_gird,1):\n",
        "    # 데이터 프레임은 매번 섞어서 훈련을 실시한다.\n",
        "    x_train, x_test, y_train, y_test = train_test_split(data_scaled, target_scaled, test_size=0.3, shuffle=True)\n",
        "\n",
        "    # 하이퍼 파라미터\n",
        "    lstm_layers, dense_layers, dropout, batch_size = params \n",
        "    #모델 생성\n",
        "    model = make_lstm(lstm_layers = lstm_layers, dense_layers=dense_layers, dropout=dropout) \n",
        "    model.summary()\n",
        "    #모델 학습 \n",
        "    model.fit(x_train, y_train, batch_size=batch_size, epochs=500, callbacks=[early_stopping, reduce_lr], validation_data=(x_test, y_test))\n",
        "    model_name = '256same_model_opt_lstm{0}'.format(i)\n",
        "    model.save('models/'+model_name+'.h5')\n",
        "\n",
        "    # 메모리 비우기. 실행안하면 GPU 메모리 에러.\n",
        "    gc.collect()\n",
        "    \n",
        "    #train 예측 \n",
        "    train_predictions = model.predict(x_train, verbose=1)\n",
        "\n",
        "    #train 역변환 \n",
        "    val = target_scaler.inverse_transform(y_train)\n",
        "    predictions = target_scaler.inverse_transform(train_predictions)\n",
        "\n",
        "    #train RMSE\n",
        "    train_RMSE = mean_squared_error(val, predictions)**0.5\n",
        "\n",
        "    #test 예측 \n",
        "    test_predictions = model.predict(x_test, verbose=1)\n",
        "\n",
        "    #test 역변환 \n",
        "    val = target_scaler.inverse_transform(y_test)\n",
        "    predictions = target_scaler.inverse_transform(test_predictions)\n",
        "\n",
        "    #test RMSE\n",
        "    test_RMSE = mean_squared_error(val, predictions)**0.5\n",
        "    \n",
        "    result_df = pd.DataFrame({'model':[model_name], \n",
        "                          'lstm_layer':[lstm_layers],\n",
        "                          'dense_layer':[dense_layers],\n",
        "                          'dropout':[dropout],\n",
        "                          'batch_size':[batch_size],\n",
        "                          'train_RMSE':[train_RMSE],\n",
        "                          'test_RMSE':[test_RMSE]\n",
        "                         })\n",
        "    \n",
        "    # 메모리 비우기. 실행안하면 GPU 메모리 에러.\n",
        "    gc.collect()\n",
        "    \n",
        "    if not os.path.exists(result_pilename):\n",
        "        result_df.to_csv(result_pilename, index=False, mode='w')\n",
        "\n",
        "    else:\n",
        "        result_df.to_csv(result_pilename, index=False, mode='a', header=False)\n",
        "    \n",
        "    print('')\n",
        "    print(result_df)\n",
        "    print('')\n",
        "    print('-'*268)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}